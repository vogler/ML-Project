%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2010 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2010,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}
\usepackage[utf8]{inputenc}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2010, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2010} with
% \usepackage[nohyperref]{icml2010} above.
%\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
% \usepackage{icml2010} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
%\usepackage[accepted, nohyperref]{icml2010}
\usepackage[nohyperref]{icml2010}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Number recognition for use in a Sudoku solver}


% custom imports
\usepackage{amsmath}

\begin{document} 

\twocolumn[
\icmltitle{Number recognition for use in a Sudoku solver}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2010
% package.
\icmlauthor{Ralf Vogler}{voglerr@in.tum.de}
\icmladdress{TU München}
\icmlauthor{Simon Grötzinger}{groetzin@in.tum.de}
\icmladdress{TU München}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{neural network, svm, logistic regression, synthetic data generation, machine learning}

\vskip 0.3in
]

\begin{abstract}
The goal of this work is to implement number recognition for a Soduko solver.
First we will describe the generation of synthetic data, in this case labelled images of numbers.
Then we compare different machine learning techniques for supervised learning using the generated data.
Once the training is complete, images of real Sudokus are captured using the webcam.
The different methods then try to correctly predict each number.
\end{abstract}

\section{Introduction}
Number and letter recognition is key in many real-world applications. Digitalizing print media, automated sorting of letters, or recognition of signs in robotics or driver assistance systems require well performing algorithms for character recognition. In the following we analyze the performance of different techniques for number recognition, namely Neural Networks (NN), Support Vector Machines (SVM) and Logistic Regression (LR), by using them in an automated Sudoku solver.

\section{Overview}
Figure \ref{fig:overview} shows the basic data flow and the participating data manipulators. First of all we generate data by sampling position, rotation, size and color of a number and place it based on these values on a white 20x20 pixel image. Examples of 9 such generated images are shown in Figure \ref{fig:generated_numbers}.The corresponding labels are defined by the name of the folder the images are stored in.
\begin{figure}[ht]
	\centerline{\includegraphics[scale=0.25]{../examples}}
	\caption{Examples of generated numbers}
	\label{fig:generated_numbers}
\end{figure}
These images are preprocessed (applying greyscale) by an octave script and written to an input matrix and a target vector. All pixels of one image are encoded in one row of the input matrix and the corresponding label is stored in the row of the target vector. The data is dumped to a ``cache.mat'' file which allows us to apply many different tools and programming languages to the dataset. In our case we use it in python and octave.
% trim=l b r t
\begin{figure}[ht]
	\centerline{\includegraphics[width=\columnwidth, trim=25 30 25 30]{../overview}}
	\caption{Overview}
	\label{fig:overview}
\end{figure}
The input matrix $X^{n\text{x}400}$ and the target vector $y$ are the inputs for training NN, SVM and LR, as described in Section \ref{training}. After training, the learned models/parameters are saved in ``.mat'' files that can be loaded by the number prediction. The other input for number prediction is an input matrix of the same form as $X$ with a row entry for each Sudoku field that was detected to contain a number. This implies that we do not train our models to detect empty fields. We could reliably detect empty fields by performing some preprocessing steps on the images (filling borders, adjusting thresholds etc.) thus applying a machine learning technique does not seem appropriate for this task.

\section{Training}
\label{training}
\subsection{Neural Network}
Applying a NN seems promising as it was already successfully applied to the quite similar problem of detecting handwritten digits. \color{red}ref!\color{black} For the NN we choose a simple architecture with 25 hidden sigmoidal units (+ bias unit), as shown in figure \ref{fig:nn}. The size of the input layer is 400 (given by the 20 x 20 pixel images) and the size of the output layer is defined by the number of classes, in this case the numbers 1 to 9. This simple architecture turned out to perform pretty well compared to more complex ones with a higher number of hidden layers. Training is performed with error backpropagation, thus minimizing the error function
\begin{align*}
& E(w) = \sum_{n=1}^N E_n(w)\\
\text{with}~~E&_n(w) = \frac{1}{2}\sum_k (y_{nk}-t_{nk})\\
\text{and}~~~~~&~~~y_k = \sum_i w_{ki}x_{i}\\
\end{align*}
subject to $w$.
\begin{figure}[ht]
	\centerline{\includegraphics[width=\columnwidth, trim=0 30 0 30]{../nn}}
	\caption{Neural Network with one hidden layer}
	\label{fig:nn}
\end{figure}
For NNs we test two implementations. On the one hand we use a partly self-developed octave code for two-layer NNs which was written as part of the stanford machine learning course\footnote{http://www.ml-class.org}, on the other hand we include PyBrain \cite{schaul2010pybrain} for comparison and easier evaluation of more sophisticated network architectures. To find a good value for the regularization parameter $\lambda$ we perform a grid search with k-fold cross-validation which gives us a good performance when choosing $\lambda = 1$.

\subsection{Support Vector Machines}
The next technique we apply to the discussed problem are Support Vector Machines \cite{Bishop200608} which try to solve the following optimization problem:
\begin{align*}
\min_{w,b} &~~ \frac{1}{2}||w||^2 + C \sum_{i=1}^{n} \xi_i\\
\text{subject to } &~~ y_i(w^T \phi(x_i) + b) - 1 + \xi_i \ge 0\\
\text{and} &~~ \xi_i \ge 0.
\end{align*}
Using a radial basis function (RBF) as kernel we perform a grid search over $C$ and $\gamma$ to find those parameters that provide highest accuracy on the test set. We use the open source library ``libsvm'' \cite{chang2011libsvm} wrapped by some octave scripts for setting up data sets and performing grid search with k-fold cross-validation. Grid search with cross validation resulted in accuracies illustrated in Figure \ref{fig:svm_grid_search}. From the best parameter pairs we choose the one that performs best on the validation set, which gives us $C = 20$ and $\gamma = 0.1$.
\begin{figure}[ht]
	\centerline{\includegraphics[scale=0.20, trim=0 0 10 0]{../gridSearchSVMPlot}}
	\caption{}
	\label{fig:svm_grid_search}
\end{figure}

\subsection{Logistic Regression}
Finally we apply Logistic Regression to the datasets. Again we use a (vectorized) octave implementation for learning and prediction. Therefore we minimize the (regularized) error function
\begin{align*}
E(w) &= \frac{1}{n}(-y^T ln(\Phi w) + (1-y^T)ln(1-\Phi w))\\
&~~~+\frac{\lambda}{2n}w^Tw
\end{align*}
with respect to $w$. Minimization is performed by an algorithm\footnote{http://sprinkler.googlecode.com/svn/trunk/\\regression/fmincg.m} that was provided during the stanford machine learning course. Grid search and k-fold cross-validation recommend a value for the regularization parameter $\lambda$.

\section{Image Recognition and Preprocessing}
Image recognition is written in C by extensive use of the open source library OpenCV\footnote{http://opencv.willowgarage.com/wiki/}. Images captured by the webcam are continously analyzed to find a Sudoku square. Images are transformed to greyscale, then an adaptive threshold algorithm is used to preprocess the image for contour detection by OpenCV. OpenCV returns a list of contours which are further checked for certain criteria like having four corners or being of certain size. Since the contours are not detected precise enough lines are fitted through six checkpoints on each edge in order to achieve subpixel accuracy. The content enclosed by the corners is perspectively warped to fit a predefined square size. Finally, the thresholded Sudoku field is saved to disk. A python script is then used to split the whole field into 9x9 subfields with each subfield being flood filled around the corners for noise reduction. The format of the subfields now equals to the one provided by the generated images which allows us to apply the trained NN/SVM/LR models and, finally, apply a Sudoku solver.

\section{Evaluation}
To evaluate the performance of different approaches we define the following data sets:
\begin{itemize}
\item d1: 4500 generated images
\item d2: $\sim$ 1000 gathered images
\item d3: D1 $\cup$ D2
\item dVal: images of a captured Sudoku
\end{itemize}

\color{red}
\begin{table}[t]
\caption{Classification accuracies for Neural Network, SVM and Logistic Regression on different data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\hline
\abovespace\belowspace
Data set & NN & SVM & LR & Winner \\
\hline
\abovespace
D1		& 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& 96.7$\pm$ 0.2& ?\\
D2		& 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& ? & ?\\
D3		& 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& ? & ? \\
\belowspace
DVal	& 74.8$\pm$ 0.5& 78.3$\pm$ 0.6& ? & ?        \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\color{black}

\section{Future Work}
\color{red}
\begin{itemize}
\item include information if a sudoku is unsolvable
\item online learning
\end{itemize}
\color{black}


\section{References}

\bibliography{paper}
\bibliographystyle{icml2010}

\end{document}  


